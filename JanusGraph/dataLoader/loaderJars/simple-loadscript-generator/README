Stilton: HIL 2 Titan Graph Ingestion Code Generator
===================================================
Release 2.4
-----------
Features:
- - - - - 
  1. Titan-0.9/TP3 support
  -  -  -  -  -  -  -  -  -
  The schema and data ingestion gremlin scripts generated by this release 
  are fully compliant with the Titan-0.9-M2 and TinkerPop3. They have been
  tested on Titan-0.9-M2-hadoop1 release. 

  !!!!!!!!!     WARNING      !!!!!!!!!!!!!!
      Unfortunately, at the time of this release, the Hadoop-based parallel
      bulkloading within Titan is broken. As a result, we have withdrawn the
      feature for generating necessary Gremlin code. 

  2. Edge-centric annotations
  -  -  -  -  -  -  -  -  -  - 
  In addition to the vertex-level annotations supported in the previous 
  release, 
  * Edge:
    Annotation formats:
        * @graphLink("label", "targetVertexLabel", "targetVertexProperty")

    This annotation helps to define edges which are not implicitly defined as part 
    of the nested structure of the HIL schema. Single fields within HIL entities 
    when annotated with @graphLink, define the formation of an edge between the 
    every instance of the vertex corresponding to the current HIL entity and the 
    vertex (or vertices) of type "targetVertexLabel" whose "targetVertexProperty" 
    equals the value of the annotated field of the vertex. The edge thus defined 
    will have the "label".

    Note that "targetVertexLabel" and "targetVertexProperty" should be 
    consistent with the vertex label and vertex property ** after ** applying 
    any of the @graphVertex and @graphVertexProperty annotations described above.


Release 2.3-annotations
-----------------------
Annotating the HIL Schema
- - - - - - - - - - - - -

Several examples of valid and invalid annotations within HIL schema are provided
in src/test/resources folder.

The following annotations can be applied in the HIL schema, for designing the
ingestion into Titan:

* Ignore:
    Annotation format:
        * @graphIgnore

    An entity / property in HIL can be ignored from being ingested into Titan,
    with a graphIgnore annotation. If a higher-level entity is set to graphIgnore,
    all of its sub-entities are ignored as well.

    For example, to ignore the 'city' field in the entity, it can be annotated
    with a @graphIgnore. Rest of the entities 'state' and 'country' are still
    ingested.

    declare LocationType : [
        city : string @graphIgnore,
        state : string,
        country : string
    ] @graphVertex;

* Vertex:
    Annotation formats:
        * @graphVertex
        * @graphVertex("label")
        * @graphVertex("label", "key")
        * @graphVertex(label = "" , key = "")

    An entity in a HIL schema is by default annotated as a vertex in the
    knowledge graph. And, all named / unnamed entites of type set or record are
    implicitly annotated as a vertex, unless specified otherwise. Single fields
    within HIL entities can also be transformed into a vertex in the graph space.
    If no label is specified in the annotation, the identifier name of the entity /
    field is assumed to be its label. Multiple identifiers with the same name
    cannot be ingested into the KG --- the user has to disambiguate them by passing
    the label name in the annotations.

    In the following example, the 'Person' entity in HIL is mapped to a
    'Citizen' vertex in KG, with the 'id' field being the primary key of 'Citizen'.
    There is an implicit edge with label 'residence' between 'Citizen' vertex and
    'LocationType' vertex.

    declare Person : [
        id : long,
        residence : LocationType
    ] @graphVertex(label = "Citizen", key = "id");

* Vertex Properties:
    Annotation formats:
        * @graphVertexProperty
        * @graphVertexProperty(label = "")
        * @graphVertexProperty("label")
    
    An *atomic* field within a HIL entity is by default stored as a property of
    that entity. A top-level entity declared in HIL cannot be exist as a property.
    Unnamed records or fields of type set cannot be collapsed into a single graph
    property as well.  Similar to vertices, all properties must be uniquely
    labeled.

    declare LocationType : [
        city : string @graphIgnore,
        state : string,
        country : string @graphVertexProperty("nation")
    ] @graphVertex;



Release 2.2-hadoop (works with titan-0.5.4/tp2)
------------------
In this release, the generated Gremlin code is useful in bulkloading
the CSV files into Titan graph using Titan-Hadoop across a cluster
of machines in parallel. It extends the release 2.1 by providing
an additional flag (-h) to indicate if the code generated should be
Titan-Hadoop compliant or not.

Apart from this, overall usage and capabilities of Stilton remains 
the same as the release 2.1. 

The key differences when using the Titan-Hadoop for loading the 
CSV into Titan are as follows:

Input CSV Format:
- - - - - - - - - 
All CSV files that need to be loaded into Titan should be placed 
under a single directory on HDFS (say under '/data'). Each file is 
assumed to be a valid CSV file with the following properties:
	- no Header rows 
	- each line has the name of the type/entity/filename as the first
	  column
For example, the file Profile.csv which would have looked without
Titan-Hadoop option as

   PK,id,name,company,age,salary
   1,100468,John,IBM,28,100000
   2,166448,Doe,IBM,35,200000

is formatted now as:
	Profile,1,100468,John,IBM,28,100000
	Profile,2,166448,Doe,IBM,35,200000

(Note that the bridge tables have <src_entityname>.x.<tgt_entityname> format)

Procedure for executing the Hadoop job:
- - - - - - - - - - - - - - - - - - - - 
1. Run the HIL to Gremlin code generator with a -h switch. It results in 
   schema.gremlin and reader.gremlin files
2. Execute schema.gremlin on the Titan instance to ensure all schema elements 
   are set up correctly
3. copy reader.gremlin to hdfs (say as readers/reader.gremlin) and copy all 
   the CSV files to hdfs (say under data/ directory)
4. modify the example configuration parameters in (script-hbase-bulkload.properties) 
   to point to this file and directory on HDFS.
5. open a gremlin shell, open a HadoopGraph with the configurations (from step 4), 
   and do a null action (g._()) on the graph. It compiles into a series of mapreduce 
   jobs which first write out all the vertices, then write out valid edges between 
   them, aggregates all the properties/edges of the same node, and writes them to Titan.

(
 In a simple benchmark run on a standalone Hadoop/HBase running on a 
 MacBook Pro - 2.5GHz i7, 16GB, 500GB flash, we were able to load about 6 million 
 entities and 1.6 million edges between these entities in less than 30 minutes end-to-end.
)

Release 2.1
-----------
In this release, the Gremlin code that is generated by Stilton is
capable of:
	- handling arbitrarily nested HIL schema without annotations
	- ingesting data from a set of CSV files into Titan graph
	
The rest of this readme provides info on running the Stilton code,
the structure of CSV input files, and the way to execute the generated
Gremlin code. Please note that this is not a detailed documentation,
and is not expected to be comprehensive. You are welcome to contact
either 

	Srikanta Bedathur (sbedathur@in.ibm.com) or 
	Sriram Lakshminarasimhan (sriram.ls@in.ibm.com) 
for any further assistance. Read on.

Running Stilton on a HIL file:
- - - - - - - - - - - - - - - -
The software is packaged as stilton.jar and all the dependent
JAR files are under the associated directory stilton_lib. You
can find them in the same directory as this README file (unless
you move it!).

You can execute stilton and generate a Gremlin code to ingest
data under the test folder as follows:

	$ java -cp "stilton_jar/*" -jar stilton.jar test/sample.hil test/sample.properties

It prints a long Gremlin code, which can be redirected to an
appropriate file for inspection. And it is also written to 
two files: 
	(i) graph schema portion is written to a file given against 
	    stilton.csv.output.schemafile in the stilton.csv.output.dir
	(ii) graph loading from CSV files is written to a file given
	    stilton.csv.output.scriptfile in the stilton.csv.output.dir

(Please go through the sample.properties file to know about other 
properties that can be configured.)


Input CSV data file:
- - - - - - - - - -
There are two forms of CSV files to load data from -- independent and bridge CSV files.
	

independent CSV files:
-  -  -  -  -  -  -  -
Each independent CSV file contains a primary key column followed
by the properties contained in the corresponding entity definition
in the HIL schema. Only the basic property types (i.e., string,
double, int) are expected to be in the CSV file. Every independent
CSV file also must have a header row, which matches with the property
names defined in the HIL schema.
	==> At this point, this is a strict requirement.

NOTE: the naming convention for independent CSV files is fairly
straightforward -- it is best to learn from the examples given in
the test folder.
	==> This naming convention is a strict requirement.


bridge CSV files:
-  -  -  -  -  -
These are essentially the "edge" definitions -- providing the primary
key values of the source and target entity. The naming convention
follows the pattern --
<sourceentity_independentfile>.x.<targetentity_independentfile>



Using the Gremlin code with Titan
- - - - - - - - - - - - - - - - - 
The generated gremlin code is compliant with titan-(>=0.5.0). 
It also makes use of Apache Commons-CSV library to parse the CSV
input files. Therefore commons-csv.jar should be included in the CLASSPATH
before invoking Gremlin command shell to load the scripts.

The script files expect a titan property file as an argument. You may face issues
with some versions of titan-0.5.x in handling command line arguments
(this is a known issue with gremlin.sh packaged with some titan
versions). You can execute the gremlin commands using
	$ gremlin.sh output/{schema|reader}.gremlin <titan-propertiesfile>

	OR 
you may edit the gremlin file to replace commandline argument
with the name of the titan properties file, and then execute as ...

	$ gremlin.sh
		
		 \,,,/
		 (o o)
	-----oOOo-(_)-oOOo-----
	gremlin> load {schema|reader}.gremlin 

This should load the data represented in CSV files into titan
knowledge graph. 


Configuration Properties
- - - - - - - - - - - - - 
/* Configure the delimiter symbol. It cannot be 
   _ and ""
*/
stilton.csv.delimiter =,

/* true/false to indicate if the CSV file has headers or not.
   If no headers, it is expected to strictly follow the same 
   property ordering as defined in the hil schema          
*/
stilton.csv.header=true

/* Location of CSV files, their suffix, bridge-indicator symbol, and 
   primary key indicator -- useful only when header=true above 
*/
stilton.csv.input.dir=/Users/bedathur/work/MDM/code-releases/phase2.1/test/data
stilton.csv.input.filesuffix=csv
stilton.csv.input.bridgeindicator=.x.
stilton.csv.input.primarykeysuffix = _PK

/* Location to output the resulting gremlin scripts to, and also configurable commit
   frequency useful while handling large CSV files 
*/
stilton.csv.output.dir=/Users/bedathur/work/MDM/code-releases/phase2.1/test/output
stilton.csv.output.commitfrequency=1000
stilton.csv.output.schemafile=schema.gremlin
stilton.csv.output.scriptfile=reader.gremlin

