% Chapter Template

\chapter{Setting things up} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


This chapter describes how to set up the development environment. It describes all the preliminary tasks that are to be done before we actually even start thinking anything about the in-graph index data-structures. Issues ranging from loading the dataset to the conversion of the English query description to gremlin are described and the query. We further briefly give details of our java codebase that comes in very handy while dealing with this process and streamlines the process of obtaining query timings (and other statistics) into running a fixed set of predefined classes. We also mention the problems faced during each of the tasks so as to give the read an idea of how to deal with the same, should he/she come across similar problems. This chapter is divided into the various sections for clarity. We first describe the basis for the choice of the graph database to perform experiments, then briefly talk about the query language used in the project, the third section is to help the interested reader on how to get the dataset loaded into the graph database (as this can be tricky in several cases). We finally talk about how to translate the English descriptions of the query from the LDBC benchmark into their gremlin equivalents.

\section{Choice of the Graph Database}

\subsection{Neo4j}
Neo4j being one of the most popular Graph DB, was the first choice for testing the suggested changes. Also Neo4j was initially chosen by us due to the fact that it supports both Cypher and Gremlin. Hence, we would be able to compare the performance of the Gremlin queries (both with and without our newly created Indexes) and the ones written in Cypher.

\subsection{Janus Graph}
JanusGraph is another popular Graph Database. It is an open source project and is under The Linux Foundation. It also has active contributions from organizations like Google, Amazon and IBM. JanusGraph does not have any of its own exclusive query language. Instead, it utilizes the popular Apache TinkerPop which is a one stop solution for most graph processing applications.
\\\\
The Gremlin DSL is the prime choice for the project since it is more closer written to the imperative fashion than the sql like declarative fashion. This would allow us to manually write traversals using our modifications without directly messing with the code of the core database. Though we started with Neo4J, its quickly out-dating gremlin plugins and downgrading support for Gremlin DSL (which was an important part to begin our project) implied that we shift to the other popular option of Janusgraph. Thus, after encountering some integration difficulties between Neo4j and Gremlin, we decided to move to JanusGraph.\\

\section{Query Language: Interacting with a graph database}
The two chief candidates for the choice of the query language were Cypher and Gremlin. Cypher is the query language used in Neo4j, the most popular Graph Database. Being declarative, it is closer to SQL, thus making it easier to use. Moreover, it is more advanced than most Graph DB query languages by virtue of having a query planner to help in optimizing the evaluation of the queries.
\\\\
Gremlin is an open source, vendor agnostic, graph computing frame work. It allows for both, declarative and imperative style queries on graphs. Unlike Cypher, the query planner that Gremlin has is very basic in its functionality. So it is the responsibility of the user to write the best possible way of traversal of the graph.
\\\\
From the above description, it seems that Cypher is a hands down winner among the two due to ease of use. Still, we decided to go ahead with Gremlin, owing primarily to the fact that Gremlin supports querying all major Graph DBs but Cypher only supports Neo4j. We must also note that Gremlin provides better flexibility to the user (due to the possibility of writing imperative style queries) in terms of deciding the evaluation plan rather than leaving it up to the query planner. This was a key part of the project since it involved modifying the graph traversals to use different kinds of indexes available. We needed a highly granular control over the traversal. This reason overruled all others and we chose to use the gremlin query language\cite{tinkerpop1}.


\section{Loading the dataset}

Using the LDBC datagen \cite{datagen} we generated the dataset with standard scale factor (SF=1) which created about 1GB of dataset in csv files. We used the default berkeley DB backend to start with, which we eventually changed to the Cassandra backend where we observed an improvement in terms of dataset loading times. Since being able to change and load the dataset was crucial to be able to conduct the experiments, we ended up sticking to the Cassandra backend.
\\\\
The Janusgraph configuration was set to turn on batch loading with the configuration `storage.batch-loading=true` and `schema.default = none' (Note: This turns off the transactions in the graph database and so concurrent writes might cause inconsistencies). Turning off schema auto creation is required as mentioned in the JanusGraph documentation to turn on the batch loading. We use the jars given along with the project code in the repository and then insert the data points using a simple for loop. We flush the memory to enforce that the DB writes to the disk after every 30,000 data points insertion and allow the process to do a garbage collection manually. Without this we faced several crashes and severe slow downs in many cases.
\\
The general load times that we observed for this Scale Factor=1 dataset (which we also call the 1GB dataset) is as shown in the following table. The size of the bloated dataset was around 6.7GB.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Machine} & \textbf{Dataset Size} & \textbf{Loading Time} \\
\hline
& &\\
Baadal & 120 MB & \textasciitilde 40 mins \\
(32 GB, 8 cores) & & \\
\hline
& &\\
Aryabhata & 1GB & \textasciitilde 2 hours \\
(128 GB, 32 cores) & & \\
\hline
\end{tabular}
\end{center}
$\:$\\
One important thing to note here is that the required elastic search indexes for the experiments were also created before loading data during the creation of the schema (we had to do this manually since schema.default=None was set in the configuration). This was done because the re-indexing step required when creating a new index on a pre-loaded dataset always timed out even with substantially large time out limits (in order of days).\\
\\
Now that we have our dataset in place, we can start looking at the query workloads and their implication for our proposed idea. This is exactly what the next section offers.

\section{Working with queries in LDBC}

LDBC's SNB Business Intelligence workload is a set of complex structured queries analyzing the behaviour of entities on a social network. These are parameterized with certain substitution parameters (eg. Get all persons below age A, where A is an parameter), to be able to obtain the same information in various parts of the social network. These substitution parameters can be secured from the LDBC data generator that also generates parameters. The queries are given in the LDBC SNB specification \cite{ldbcQueries} are described in simple English with expected inputs and the outputs (along with the sort order and retrieval). Naturally the first step, before testing the proposed in graph index data structure, boils down to translating these queries into the Gremlin DSL. This will eventually help us compare the raw query time (or time of the best raw query with the SOTA existing index structure) to the performance that our index structure attains.\\
\\
For flexibility, ease of use and better reproducibility we have maintained a Java codebase \cite{repo} that contains the gremlin equivalent of almost all the LDBC BI queries (without the ordering steps). These query classes are contained in the Queries module of the project. We have tried to make the interface generic for each query to allow for easy computation  of any timing statistics that the query might require. More details can be found on the wiki and the readme of the code repository.\\
\\
One sample query from the LDBC SNB BI and its corresponding gremlin equivalent (without the order step) is shown below to describe the general conversion. The queries keep getting more and more complex as we long further in the DI workload and the reader can also look at the gremlin documentation to understand the various steps used in the more complicated gremlin query.\\
\\
\begin{center}
  \begin{minipage}{0.8\textwidth}
    Given a date, find all Messages created before that date. Group them by a 3-level grouping:
    \begin{enumerate}
    \item by year of creation
    \item for each year, group into Message types: is Comment or not
    \item for each year-type group, split into four groups based on length of their content
      \begin{itemize}
      \item 0 <= length < 40 (short)
      \item 40 <= length < 80 (one liner)
      \item 80 <= length < 160 (tweet)
      \item 160 <= length (long)
      \end{itemize}
    \end{enumerate}
  \end{minipage}
\end{center}
$\:$\\
\begin{lstlisting}
g.V().hasLabel("post", "comment")
  .has("po_creationDate", P.lt(dateVar))
  .group()
  .by{ it.value("po_creationDate").getYear()+1900 }
  .by(group().by(T.label).by(group().by{
    int len = it.value("length");
    if (len < 40) {
        return "short";
    } else if (len < 80) {
        return "one liner";
    } else if (len < 160) {
        return "tweet";
    } else {
        return "long";
    }}));
\end{lstlisting}


